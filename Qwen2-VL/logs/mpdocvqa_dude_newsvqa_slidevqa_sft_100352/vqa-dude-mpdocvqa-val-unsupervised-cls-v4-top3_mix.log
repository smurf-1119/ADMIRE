+ CHECKPOINT=/gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
+ DATASET=vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix
+ GPUS=8
+ CHECKPOINT=/gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
++ pwd
+ export PYTHONPATH=/mntnlp/zqp/physical_report/Qwen2-VL:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public
+ PYTHONPATH=/mntnlp/zqp/physical_report/Qwen2-VL:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public
+ echo 'CHECKPOINT: /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/'
CHECKPOINT: /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
+ MASTER_PORT=62621
+ PORT=62621
+ GPUS=8
+ export MASTER_PORT=62621
+ MASTER_PORT=62621
+ export PORT=62621
+ PORT=62621
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-mpdocvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-mpdocvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-mpdocvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-mpdocvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-mpdocvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-dude-mpdocvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix ']'
+ torchrun --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --nproc_per_node=8 --master_port=62621 infer_mpdocvqa_dynamic_unsupervised_cls4_multigpu.py --checkpoint /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/ --datasets dude_mpdocvqa_val --max-pixels-low 200704 --max-pixels-high 802816 --out-dir ./res/dude-mpdoc-vqa --dynamic --select-mode mix_topk --select-image-num 3
datasets: ['dude_mpdocvqa_val']
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
datasets: ['dude_mpdocvqa_val']
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
datasets:datasets:  ['dude_mpdocvqa_val']['dude_mpdocvqa_val']

datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.48s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.53s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.90s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.96s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.99s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.41s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.55s/it]0it [00:00, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
0it [00:00, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]
0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]1it [00:06,  6.43s/it]1it [00:06,  6.55s/it]1it [00:06,  6.45s/it]1it [00:06,  6.88s/it]1it [00:07,  7.18s/it]2it [00:09,  4.27s/it]1it [00:07,  7.74s/it]2it [00:08,  4.04s/it]2it [00:08,  3.72s/it]2it [00:08,  3.76s/it]1it [00:08,  8.06s/it]1it [00:08,  8.30s/it]2it [00:08,  3.86s/it]3it [00:10,  3.00s/it]3it [00:10,  3.07s/it]3it [00:10,  2.92s/it]3it [00:10,  2.99s/it]2it [00:10,  4.66s/it]2it [00:10,  4.78s/it]3it [00:10,  3.11s/it]2it [00:10,  4.86s/it]4it [00:12,  2.48s/it]4it [00:12,  2.36s/it]4it [00:11,  2.23s/it]4it [00:11,  2.36s/it]3it [00:11,  3.31s/it]3it [00:12,  3.38s/it]4it [00:12,  2.59s/it]3it [00:12,  3.48s/it]4it [00:13,  2.50s/it]4it [00:13,  2.61s/it]5it [00:15,  2.79s/it]4it [00:14,  2.98s/it]5it [00:15,  2.90s/it]5it [00:15,  2.81s/it]5it [00:15,  2.87s/it]5it [00:15,  2.75s/it]5it [00:16,  2.91s/it]6it [00:19,  3.16s/it]6it [00:19,  3.06s/it]5it [00:18,  3.32s/it]6it [00:19,  3.15s/it]5it [00:18,  3.34s/it]6it [00:19,  3.13s/it]6it [00:19,  3.12s/it]7it [00:21,  2.63s/it]7it [00:21,  2.72s/it]6it [00:20,  3.13s/it]8it [00:22,  2.12s/it]7it [00:20,  2.76s/it]7it [00:21,  2.73s/it]8it [00:22,  2.10s/it]7it [00:21,  2.74s/it]8it [00:21,  2.11s/it]6it [00:21,  3.29s/it]6it [00:21,  3.19s/it]8it [00:22,  2.15s/it]9it [00:23,  1.83s/it]7it [00:21,  2.61s/it]8it [00:22,  2.10s/it]9it [00:23,  1.81s/it]9it [00:23,  1.86s/it]10it [00:24,  1.62s/it]8it [00:22,  2.10s/it]7it [00:23,  2.80s/it]9it [00:23,  1.98s/it]10it [00:24,  1.65s/it]9it [00:23,  1.92s/it]10it [00:23,  1.53s/it]7it [00:23,  2.78s/it]11it [00:25,  1.47s/it]8it [00:24,  2.15s/it]9it [00:24,  1.79s/it]10it [00:24,  1.66s/it]8it [00:24,  2.15s/it]10it [00:24,  1.61s/it]11it [00:25,  1.51s/it]10it [00:24,  1.50s/it]11it [00:25,  1.52s/it]9it [00:25,  1.90s/it]11it [00:26,  1.58s/it]9it [00:26,  1.98s/it]11it [00:26,  1.63s/it]10it [00:26,  1.64s/it]11it [00:26,  1.52s/it]10it [00:26,  1.65s/it]11it [00:27,  1.54s/it]11it [00:28,  1.48s/it]12it [00:30,  2.38s/it]12it [00:29,  2.31s/it]12it [00:29,  2.29s/it]12it [00:29,  2.23s/it]13it [00:31,  1.95s/it]13it [00:30,  1.87s/it]12it [00:30,  2.25s/it]13it [00:30,  1.89s/it]13it [00:30,  1.83s/it]12it [00:30,  2.23s/it]13it [00:31,  1.91s/it]13it [00:31,  1.88s/it]12it [00:31,  2.26s/it]12it [00:32,  2.24s/it]13it [00:33,  2.04s/it]13it [00:33,  2.04s/it]14it [00:36,  2.97s/it]14it [00:36,  3.03s/it]14it [00:35,  3.01s/it]15it [00:37,  2.31s/it]14it [00:36,  2.95s/it]15it [00:37,  2.34s/it]14it [00:36,  2.95s/it]15it [00:36,  2.34s/it]15it [00:37,  2.43s/it]14it [00:37,  3.15s/it]15it [00:37,  2.48s/it]15it [00:38,  2.52s/it]14it [00:39,  3.20s/it]16it [00:40,  2.72s/it]14it [00:39,  3.22s/it]16it [00:40,  2.78s/it]15it [00:40,  2.58s/it]17it [00:41,  2.21s/it]16it [00:41,  2.93s/it]15it [00:40,  2.66s/it]17it [00:42,  2.26s/it]16it [00:41,  2.91s/it]16it [00:42,  2.96s/it]17it [00:42,  2.39s/it]17it [00:42,  2.34s/it]17it [00:42,  2.24s/it]16it [00:42,  2.92s/it]17it [00:43,  2.22s/it]16it [00:43,  2.87s/it]17it [00:44,  2.20s/it]16it [00:44,  2.93s/it]18it [00:46,  2.93s/it]17it [00:45,  2.26s/it]18it [00:46,  2.95s/it]18it [00:46,  2.86s/it]18it [00:46,  2.82s/it]18it [00:46,  2.78s/it]18it [00:47,  2.95s/it]19it [00:50,  3.19s/it]19it [00:50,  3.25s/it]19it [00:49,  3.17s/it]18it [00:50,  3.23s/it]20it [00:51,  2.67s/it]19it [00:50,  3.24s/it]20it [00:51,  2.58s/it]19it [00:51,  3.29s/it]20it [00:51,  2.56s/it]18it [00:50,  3.29s/it]20it [00:52,  2.72s/it]19it [00:52,  3.38s/it]20it [00:52,  2.77s/it]21it [00:54,  2.64s/it]21it [00:54,  2.64s/it]21it [00:53,  2.63s/it]20it [00:53,  2.88s/it]22it [00:56,  2.40s/it]21it [00:55,  2.75s/it]19it [00:54,  3.68s/it]22it [00:55,  2.32s/it]22it [00:55,  2.32s/it]21it [00:55,  2.84s/it]20it [00:55,  2.85s/it]22it [00:56,  2.29s/it]19it [00:56,  3.85s/it]21it [00:56,  2.80s/it]23it [00:58,  2.28s/it]22it [00:57,  2.43s/it]23it [00:57,  2.24s/it]23it [00:57,  2.30s/it]20it [00:57,  3.14s/it]22it [00:57,  2.38s/it]23it [00:58,  2.38s/it]21it [00:58,  2.85s/it]24it [01:00,  2.25s/it]23it [00:59,  2.41s/it]24it [01:00,  2.35s/it]22it [00:59,  2.32s/it]21it [00:59,  2.84s/it]25it [01:01,  1.88s/it]24it [01:00,  2.31s/it]23it [01:00,  2.37s/it]25it [01:01,  1.83s/it]24it [01:00,  2.30s/it]25it [01:01,  1.89s/it]22it [01:00,  2.32s/it]26it [01:02,  1.78s/it]24it [01:01,  2.39s/it]25it [01:02,  1.95s/it]26it [01:02,  1.75s/it]23it [01:01,  2.26s/it]24it [01:02,  2.26s/it]26it [01:02,  1.81s/it]25it [01:02,  1.98s/it]23it [01:03,  2.34s/it]25it [01:03,  1.90s/it]26it [01:03,  1.85s/it]24it [01:04,  2.27s/it]26it [01:04,  1.89s/it]27it [01:06,  2.24s/it]26it [01:05,  1.90s/it]25it [01:05,  1.94s/it]27it [01:06,  2.39s/it]24it [01:05,  2.37s/it]27it [01:06,  2.44s/it]28it [01:08,  2.20s/it]25it [01:06,  2.03s/it]26it [01:06,  1.83s/it]27it [01:07,  2.45s/it]27it [01:08,  2.38s/it]28it [01:08,  2.35s/it]28it [01:08,  2.22s/it]29it [01:09,  1.94s/it]26it [01:08,  1.86s/it]29it [01:09,  1.90s/it]27it [01:08,  2.42s/it]29it [01:09,  1.86s/it]28it [01:09,  2.28s/it]28it [01:09,  2.16s/it]30it [01:11,  1.99s/it]27it [01:10,  2.39s/it]29it [01:11,  2.08s/it]30it [01:11,  1.92s/it]28it [01:10,  2.37s/it]29it [01:11,  1.97s/it]30it [01:11,  1.96s/it]27it [01:11,  2.27s/it]31it [01:13,  1.82s/it]31it [01:13,  1.76s/it]28it [01:12,  2.18s/it]30it [01:12,  1.98s/it]29it [01:12,  2.09s/it]31it [01:13,  1.87s/it]30it [01:13,  1.94s/it]29it [01:13,  1.83s/it]28it [01:13,  2.12s/it]31it [01:13,  1.74s/it]32it [01:15,  1.95s/it]30it [01:13,  1.92s/it]32it [01:15,  1.91s/it]31it [01:14,  1.84s/it]29it [01:14,  1.87s/it]33it [01:16,  1.60s/it]30it [01:14,  1.76s/it]33it [01:15,  1.54s/it]32it [01:15,  1.98s/it]31it [01:15,  1.72s/it]32it [01:16,  1.93s/it]33it [01:16,  1.68s/it]31it [01:16,  1.60s/it]30it [01:16,  1.85s/it]33it [01:16,  1.52s/it]32it [01:16,  1.89s/it]34it [01:18,  1.73s/it]34it [01:18,  1.68s/it]32it [01:17,  1.83s/it]33it [01:17,  1.59s/it]31it [01:17,  1.72s/it]34it [01:18,  1.71s/it]33it [01:18,  1.53s/it]32it [01:18,  1.77s/it]34it [01:19,  1.71s/it]33it [01:19,  1.50s/it]35it [01:20,  1.96s/it]34it [01:19,  1.75s/it]35it [01:20,  2.03s/it]32it [01:20,  1.90s/it]34it [01:20,  1.68s/it]35it [01:20,  1.97s/it]33it [01:20,  1.54s/it]34it [01:21,  1.69s/it]35it [01:21,  2.08s/it]36it [01:24,  2.37s/it]35it [01:23,  2.26s/it]36it [01:24,  2.47s/it]35it [01:23,  2.16s/it]37it [01:25,  1.96s/it]34it [01:23,  1.91s/it]36it [01:24,  2.50s/it]37it [01:25,  2.02s/it]35it [01:24,  2.23s/it]37it [01:25,  1.98s/it]36it [01:25,  2.49s/it]36it [01:26,  2.60s/it]37it [01:26,  2.19s/it]37it [01:27,  2.09s/it]36it [01:27,  2.72s/it]35it [01:27,  2.54s/it]38it [01:29,  2.61s/it]38it [01:28,  2.47s/it]37it [01:28,  2.12s/it]36it [01:28,  2.67s/it]39it [01:30,  2.10s/it]38it [01:29,  2.53s/it]39it [01:29,  2.03s/it]37it [01:29,  2.15s/it]39it [01:29,  2.03s/it]38it [01:30,  2.56s/it]38it [01:30,  2.39s/it]40it [01:32,  2.07s/it]39it [01:31,  2.00s/it]36it [01:30,  2.73s/it]40it [01:31,  2.01s/it]39it [01:31,  1.86s/it]38it [01:31,  2.49s/it]40it [01:31,  2.00s/it]37it [01:31,  2.15s/it]38it [01:32,  2.33s/it]39it [01:32,  1.94s/it]40it [01:32,  1.87s/it]40it [01:32,  1.78s/it]39it [01:32,  1.82s/it]41it [01:34,  2.16s/it]41it [01:33,  2.06s/it]41it [01:34,  2.08s/it]40it [01:33,  1.89s/it]42it [01:35,  1.86s/it]42it [01:35,  1.77s/it]38it [01:34,  2.31s/it]42it [01:34,  1.62s/it]41it [01:34,  1.96s/it]40it [01:34,  1.78s/it]41it [01:35,  1.91s/it]39it [01:34,  1.82s/it]42it [01:35,  1.63s/it]42it [01:36,  1.65s/it]41it [01:36,  2.07s/it]43it [01:37,  2.06s/it]43it [01:38,  2.13s/it]43it [01:37,  1.89s/it]40it [01:36,  1.90s/it]41it [01:37,  2.06s/it]42it [01:37,  1.69s/it]43it [01:37,  1.75s/it]43it [01:38,  1.81s/it]42it [01:38,  1.87s/it]44it [01:40,  2.18s/it]44it [01:40,  2.26s/it]43it [01:39,  1.84s/it]44it [01:39,  2.10s/it]44it [01:39,  1.91s/it]41it [01:39,  2.15s/it]44it [01:40,  1.88s/it]45it [01:41,  1.82s/it]45it [01:42,  2.01s/it]43it [01:40,  2.05s/it]45it [01:41,  1.95s/it]45it [01:41,  1.81s/it]42it [01:41,  1.93s/it]46it [01:42,  1.59s/it]46it [01:42,  1.59s/it]45it [01:41,  1.77s/it]44it [01:41,  1.97s/it]46it [01:42,  1.62s/it]46it [01:42,  1.52s/it]46it [01:42,  1.55s/it]45it [01:43,  1.89s/it]44it [01:43,  2.20s/it]43it [01:43,  2.19s/it]46it [01:44,  1.59s/it]47it [01:45,  2.08s/it]45it [01:44,  1.89s/it]47it [01:46,  2.13s/it]47it [01:45,  2.11s/it]47it [01:45,  2.03s/it]47it [01:45,  2.00s/it]46it [01:45,  1.61s/it]44it [01:45,  2.09s/it]48it [01:47,  2.06s/it]48it [01:48,  2.19s/it]47it [01:47,  2.08s/it]48it [01:47,  2.21s/it]45it [01:47,  2.04s/it]48it [01:48,  2.19s/it]48it [01:48,  2.15s/it]46it [01:48,  1.68s/it]47it [01:48,  2.05s/it]48it [01:49,  2.02s/it]48it [01:51,  2.32s/it]49it [01:52,  3.02s/it]49it [01:53,  3.02s/it]47it [01:52,  2.27s/it]49it [01:52,  3.05s/it]50it [01:53,  2.46s/it]49it [01:53,  3.14s/it]49it [01:53,  3.02s/it]50it [01:54,  2.52s/it]50it [01:54,  2.47s/it]49it [01:54,  2.92s/it]50it [01:54,  2.61s/it]48it [01:54,  2.27s/it]51it [01:55,  2.23s/it]50it [01:54,  2.58s/it]51it [01:56,  2.22s/it]49it [01:54,  2.62s/it]51it [01:55,  2.14s/it]50it [01:55,  2.34s/it]52it [01:56,  1.81s/it]51it [01:56,  2.20s/it]52it [01:57,  1.77s/it]52it [01:56,  1.76s/it]51it [01:56,  2.24s/it]50it [01:56,  2.32s/it]52it [01:57,  1.83s/it]51it [01:57,  2.22s/it]52it [01:57,  1.92s/it]52it [01:58,  1.89s/it]51it [01:58,  2.19s/it]53it [01:59,  2.22s/it]49it [01:58,  2.86s/it]53it [02:00,  2.30s/it]52it [01:59,  1.82s/it]53it [01:59,  2.32s/it]53it [02:00,  2.27s/it]50it [02:00,  2.44s/it]53it [02:00,  2.33s/it]53it [02:01,  2.22s/it]54it [02:02,  2.54s/it]51it [02:01,  2.26s/it]54it [02:03,  2.48s/it]52it [02:02,  1.79s/it]53it [02:02,  2.25s/it]54it [02:03,  2.59s/it]54it [02:03,  2.50s/it]55it [02:04,  2.11s/it]55it [02:04,  2.06s/it]54it [02:03,  2.41s/it]55it [02:04,  2.14s/it]54it [02:03,  2.30s/it]56it [02:05,  1.77s/it]55it [02:04,  2.08s/it]56it [02:05,  1.72s/it]55it [02:04,  2.09s/it]56it [02:05,  1.88s/it]57it [02:06,  1.62s/it]56it [02:05,  1.82s/it]55it [02:05,  2.03s/it]54it [02:05,  2.35s/it]57it [02:06,  1.60s/it]56it [02:05,  1.77s/it]53it [02:05,  2.14s/it]57it [02:06,  1.62s/it]56it [02:06,  1.80s/it]55it [02:06,  2.05s/it]57it [02:07,  1.69s/it]57it [02:07,  1.64s/it]56it [02:07,  1.74s/it]57it [02:07,  1.59s/it]58it [02:09,  1.88s/it]58it [02:09,  1.94s/it]58it [02:08,  1.89s/it]57it [02:08,  1.60s/it]54it [02:09,  2.52s/it]58it [02:09,  1.89s/it]59it [02:10,  1.72s/it]58it [02:09,  1.94s/it]59it [02:10,  1.89s/it]58it [02:10,  1.92s/it]55it [02:10,  2.22s/it]59it [02:11,  1.99s/it]59it [02:11,  1.92s/it]59it [02:12,  2.03s/it]56it [02:12,  2.02s/it]58it [02:12,  2.08s/it]59it [02:12,  2.03s/it]57it [02:13,  1.86s/it]60it [02:15,  2.58s/it]59it [02:14,  2.16s/it]60it [02:15,  2.84s/it]60it [02:16,  2.99s/it]60it [02:16,  2.95s/it]58it [02:16,  2.26s/it]60it [02:17,  2.99s/it]60it [02:17,  2.80s/it]61it [02:18,  2.83s/it]61it [02:19,  3.00s/it]62it [02:20,  2.46s/it]59it [02:18,  2.23s/it]61it [02:19,  2.96s/it]61it [02:19,  2.88s/it]62it [02:20,  2.39s/it]60it [02:19,  2.95s/it]61it [02:20,  3.04s/it]62it [02:20,  2.42s/it]61it [02:20,  2.94s/it]62it [02:20,  2.50s/it]62it [02:21,  2.47s/it]63it [02:23,  2.57s/it]62it [02:21,  2.46s/it]63it [02:23,  2.65s/it]61it [02:22,  3.04s/it]60it [02:22,  2.76s/it]63it [02:23,  2.53s/it]63it [02:23,  2.57s/it]62it [02:23,  2.42s/it]63it [02:24,  2.51s/it]63it [02:24,  2.45s/it]61it [02:26,  3.04s/it]64it [02:28,  3.38s/it]63it [02:27,  2.78s/it]64it [02:29,  3.55s/it]62it [02:28,  2.59s/it]64it [02:29,  3.38s/it]64it [02:29,  3.45s/it]63it [02:29,  2.37s/it]
[rank6]: Traceback (most recent call last):
[rank6]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls4_multigpu.py", line 572, in <module>
[rank6]:     evaluate_chat_model(model, processor, image_process_args, args.select_mode, args.select_image_num)
[rank6]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank6]:     return func(*args, **kwargs)
[rank6]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls4_multigpu.py", line 413, in evaluate_chat_model
[rank6]:     generated_ids = model.generate(**inputs.to('cuda'),
[rank6]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank6]:     return func(*args, **kwargs)
[rank6]:   File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2015, in generate
[rank6]:     result = self._sample(
[rank6]:   File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2965, in _sample
[rank6]:     outputs = self(**model_inputs, return_dict=True)
[rank6]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/root/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
[rank6]:     output = module._old_forward(*args, **kwargs)
[rank6]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 1787, in forward
[rank6]:     logits = logits.float()
[rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.17 GiB. GPU 7 has a total capacity of 79.35 GiB of which 2.49 GiB is free. Process 40048 has 11.67 GiB memory in use. Process 40050 has 11.67 GiB memory in use. Process 40045 has 9.65 GiB memory in use. Process 40052 has 7.51 GiB memory in use. Process 40051 has 5.48 GiB memory in use. Process 40046 has 11.67 GiB memory in use. Process 40047 has 7.51 GiB memory in use. Process 40049 has 11.67 GiB memory in use. Of the allocated memory 3.11 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
64it [02:29,  3.44s/it]63it [02:30,  2.61s/it]W1216 17:57:24.089000 140147162811264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 176163 closing signal SIGTERM
W1216 17:57:24.089000 140147162811264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 176164 closing signal SIGTERM
W1216 17:57:24.090000 140147162811264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 176165 closing signal SIGTERM
W1216 17:57:24.091000 140147162811264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 176166 closing signal SIGTERM
W1216 17:57:24.092000 140147162811264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 176167 closing signal SIGTERM
W1216 17:57:24.092000 140147162811264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 176168 closing signal SIGTERM
W1216 17:57:24.094000 140147162811264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 176170 closing signal SIGTERM
E1216 17:57:28.367000 140147162811264 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 6 (pid: 176169) of binary: /root/miniconda3/bin/python
Traceback (most recent call last):
  File "/root/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/root/miniconda3/lib/python3.10/site-packages/zhenjin_utils/import_hooks/torchrun_args_hook.py", line 28, in new_run
    return module.old_run(*func_args, **func_kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
infer_mpdocvqa_dynamic_unsupervised_cls4_multigpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-16_17:57:24
  host      : kmaker-kmaker-033145099100
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 176169)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-newsvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-newsvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-newsvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-newsvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-newsvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-slidevqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-slidevqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-slidevqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-slidevqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix == vqa-slidevqa-val-unsupervised-cls-v4-top5_mix ']'
