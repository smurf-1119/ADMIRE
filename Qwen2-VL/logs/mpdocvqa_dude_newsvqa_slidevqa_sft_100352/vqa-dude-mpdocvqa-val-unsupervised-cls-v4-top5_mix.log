+ CHECKPOINT=/gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
+ DATASET=vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix
+ GPUS=8
+ CHECKPOINT=/gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
++ pwd
+ export PYTHONPATH=/mntnlp/zqp/physical_report/Qwen2-VL:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public
+ PYTHONPATH=/mntnlp/zqp/physical_report/Qwen2-VL:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public
+ echo 'CHECKPOINT: /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/'
CHECKPOINT: /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
+ MASTER_PORT=62621
+ PORT=62621
+ GPUS=8
+ export MASTER_PORT=62621
+ MASTER_PORT=62621
+ export PORT=62621
+ PORT=62621
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-mpdocvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-mpdocvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-mpdocvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-mpdocvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-mpdocvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-dude-mpdocvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix ']'
+ torchrun --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --nproc_per_node=8 --master_port=62621 infer_mpdocvqa_dynamic_unsupervised_cls4.py --checkpoint /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/ --datasets dude_mpdocvqa_val --max-pixels-low 200704 --max-pixels-high 802816 --out-dir ./res/dude-mpdoc-vqa --dynamic --select-mode mix_topk --select-image-num 5
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets:datasets:  ['dude_mpdocvqa_val']['dude_mpdocvqa_val']

datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.07it/s]Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.66it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.85it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.41it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.18it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.14it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.06it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.91it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.55it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.87it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  5.03it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.73it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.72it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.59it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.66it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.33it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.74it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  5.26it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.95it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.94it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.81it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.92it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.65it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.95it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.43it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.26it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.97it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.97it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.83it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.98it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.94it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.67it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.02it/s]
0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]1it [00:01,  1.25s/it]1it [00:01,  1.75s/it]1it [00:02,  2.14s/it]1it [00:02,  2.34s/it]2it [00:02,  1.20s/it]2it [00:02,  1.08s/it]2it [00:02,  1.13s/it]1it [00:03,  3.25s/it]1it [00:02,  2.98s/it]3it [00:03,  1.12it/s]1it [00:03,  3.26s/it]2it [00:03,  1.49s/it]2it [00:03,  1.70s/it]4it [00:03,  1.40it/s]2it [00:03,  1.58s/it]2it [00:03,  1.84s/it]3it [00:04,  1.12s/it]5it [00:03,  1.61it/s]3it [00:04,  1.12s/it]3it [00:04,  1.54s/it]1it [00:04,  4.41s/it]3it [00:04,  1.32s/it]4it [00:04,  1.11it/s]4it [00:05,  1.02it/s]3it [00:04,  1.42s/it]3it [00:04,  1.67s/it]4it [00:05,  1.17s/it]4it [00:05,  1.11s/it]2it [00:05,  2.43s/it]4it [00:05,  1.18s/it]5it [00:05,  1.09it/s]5it [00:06,  1.04s/it]5it [00:06,  1.01s/it]3it [00:06,  1.70s/it]4it [00:06,  1.76s/it]6it [00:06,  1.16it/s]4it [00:06,  1.21s/it]5it [00:06,  1.31s/it]5it [00:06,  1.21s/it]6it [00:07,  1.48s/it]6it [00:07,  1.04it/s]6it [00:07,  1.06s/it]7it [00:07,  1.11s/it]6it [00:07,  1.31s/it]6it [00:07,  1.26s/it]7it [00:08,  1.05s/it]7it [00:08,  1.02s/it]5it [00:08,  1.50s/it]8it [00:09,  1.06it/s]7it [00:08,  1.37s/it]8it [00:08,  1.13it/s]9it [00:09,  1.34it/s]5it [00:09,  2.15s/it]9it [00:09,  1.33it/s]8it [00:09,  1.11s/it]7it [00:09,  1.51s/it]8it [00:10,  1.14s/it]8it [00:10,  1.66s/it]6it [00:10,  1.53s/it]9it [00:10,  1.12it/s]9it [00:10,  1.12s/it]6it [00:11,  2.00s/it]7it [00:11,  1.26s/it]10it [00:11,  1.16s/it]8it [00:11,  1.05it/s]10it [00:11,  1.04s/it]10it [00:11,  1.05it/s]11it [00:12,  1.03it/s]9it [00:11,  1.20it/s]9it [00:11,  1.66s/it]10it [00:12,  1.46it/s]7it [00:12,  1.80s/it]7it [00:12,  2.28s/it]11it [00:12,  1.66it/s]12it [00:13,  1.00s/it]8it [00:12,  1.41s/it]10it [00:12,  1.58s/it]11it [00:13,  1.13s/it]13it [00:13,  1.13it/s]10it [00:13,  1.62s/it]11it [00:13,  1.40s/it]14it [00:14,  1.40it/s]8it [00:13,  2.02s/it]12it [00:14,  1.20it/s]12it [00:14,  1.09s/it]9it [00:14,  1.51s/it]15it [00:14,  1.59it/s]11it [00:14,  1.52s/it]12it [00:14,  1.17s/it]13it [00:14,  1.16it/s]13it [00:14,  1.41it/s]11it [00:14,  1.42s/it]13it [00:14,  1.02it/s]12it [00:15,  1.32s/it]9it [00:15,  1.70s/it]14it [00:15,  1.28it/s]15it [00:15,  1.51it/s]10it [00:15,  1.57s/it]12it [00:15,  1.41s/it]14it [00:16,  1.16s/it]16it [00:16,  1.42it/s]17it [00:16,  1.71it/s]10it [00:16,  1.65s/it]13it [00:16,  1.25s/it]13it [00:16,  1.44s/it]15it [00:17,  1.03s/it]14it [00:17,  1.10s/it]14it [00:17,  1.37s/it]16it [00:17,  1.41s/it]18it [00:17,  1.58it/s]14it [00:17,  1.11s/it]11it [00:17,  1.41s/it]11it [00:17,  1.62s/it]16it [00:17,  1.09it/s]15it [00:17,  1.09s/it]19it [00:17,  1.70it/s]12it [00:18,  1.18s/it]15it [00:18,  1.01it/s]17it [00:18,  1.20it/s]15it [00:18,  1.18s/it]13it [00:18,  1.05it/s]18it [00:18,  1.38it/s]16it [00:18,  1.17it/s]19it [00:19,  1.67it/s]12it [00:19,  1.67s/it]17it [00:19,  1.62s/it]16it [00:19,  1.20s/it]20it [00:19,  1.04it/s]14it [00:19,  1.00s/it]20it [00:20,  1.46it/s]18it [00:20,  1.25s/it]21it [00:20,  1.28it/s]16it [00:20,  1.55s/it]17it [00:20,  1.19s/it]13it [00:20,  1.59s/it]22it [00:21,  1.15it/s]17it [00:21,  1.35s/it]21it [00:21,  1.08it/s]15it [00:21,  1.29s/it]14it [00:22,  1.62s/it]23it [00:22,  1.01it/s]16it [00:22,  1.13s/it]17it [00:22,  1.75s/it]22it [00:23,  1.10s/it]15it [00:22,  1.26s/it]24it [00:23,  1.18it/s]16it [00:23,  1.02it/s]18it [00:23,  1.63s/it]18it [00:23,  1.59s/it]19it [00:23,  1.98s/it]25it [00:23,  1.29it/s]17it [00:23,  1.15s/it]18it [00:23,  1.50s/it]17it [00:24,  1.02it/s]19it [00:24,  1.23s/it]23it [00:24,  1.22s/it]18it [00:24,  1.26it/s]19it [00:24,  1.50s/it]24it [00:25,  1.07s/it]18it [00:25,  1.23s/it]20it [00:25,  1.21s/it]26it [00:25,  1.06s/it]25it [00:25,  1.15it/s]19it [00:25,  1.85s/it]20it [00:25,  1.32s/it]19it [00:26,  1.00s/it]21it [00:26,  1.12s/it]20it [00:26,  1.46s/it]22it [00:26,  1.15it/s]21it [00:26,  1.14s/it]20it [00:26,  1.15it/s]27it [00:27,  1.28s/it]19it [00:27,  1.47s/it]23it [00:27,  1.14it/s]20it [00:28,  2.60s/it]21it [00:27,  1.46s/it]24it [00:27,  1.25it/s]22it [00:28,  1.23s/it]22it [00:28,  1.13s/it]23it [00:28,  1.04it/s]25it [00:28,  1.46it/s]28it [00:28,  1.32s/it]21it [00:28,  1.20s/it]24it [00:28,  1.19it/s]26it [00:29,  1.65s/it]20it [00:29,  1.63s/it]21it [00:29,  2.34s/it]25it [00:29,  1.35it/s]23it [00:29,  1.18s/it]21it [00:29,  1.29s/it]22it [00:30,  1.75s/it]29it [00:29,  1.34s/it]24it [00:30,  1.03s/it]30it [00:30,  1.03s/it]25it [00:30,  1.23it/s]27it [00:30,  1.59s/it]22it [00:30,  1.16s/it]22it [00:30,  1.47s/it]28it [00:30,  1.22s/it]23it [00:30,  1.08it/s]26it [00:31,  1.34it/s]26it [00:31,  1.28s/it]24it [00:32,  1.05it/s]27it [00:32,  1.20s/it]29it [00:32,  1.24s/it]23it [00:32,  1.97s/it]31it [00:32,  1.34s/it]25it [00:32,  1.24it/s]26it [00:32,  1.43s/it]28it [00:32,  1.06it/s]27it [00:32,  1.11s/it]30it [00:32,  1.11s/it]27it [00:33,  1.17s/it]23it [00:33,  1.87s/it]28it [00:33,  1.04it/s]29it [00:34,  1.27it/s]24it [00:34,  1.95s/it]32it [00:34,  1.53s/it]25it [00:34,  1.47s/it]29it [00:34,  1.28s/it]33it [00:34,  1.25s/it]31it [00:35,  1.39s/it]30it [00:35,  1.16it/s]26it [00:35,  1.38s/it]28it [00:35,  1.50s/it]24it [00:35,  1.86s/it]34it [00:35,  1.10s/it]31it [00:35,  1.27it/s]32it [00:35,  1.23s/it]25it [00:35,  1.46s/it]29it [00:36,  1.29s/it]35it [00:36,  1.13it/s]33it [00:36,  1.03it/s]30it [00:36,  1.42s/it]32it [00:36,  1.28it/s]30it [00:36,  1.09s/it]36it [00:36,  1.24it/s]26it [00:37,  1.68s/it]34it [00:36,  1.18it/s]31it [00:36,  1.15s/it]33it [00:36,  1.56it/s]31it [00:36,  1.17it/s]27it [00:37,  1.53s/it]26it [00:37,  1.39s/it]32it [00:37,  1.44it/s]34it [00:37,  1.61it/s]32it [00:37,  1.05s/it]27it [00:37,  1.46s/it]35it [00:37,  1.09it/s]28it [00:38,  1.18s/it]33it [00:38,  1.08it/s]33it [00:38,  1.27it/s]27it [00:38,  1.33s/it]36it [00:38,  1.18it/s]37it [00:38,  1.17s/it]34it [00:38,  1.28it/s]35it [00:38,  1.12it/s]28it [00:39,  1.17s/it]38it [00:39,  1.02s/it]35it [00:39,  1.32it/s]29it [00:39,  1.09it/s]34it [00:39,  1.01it/s]28it [00:39,  1.92s/it]37it [00:39,  1.00s/it]29it [00:40,  1.35s/it]35it [00:40,  1.25it/s]39it [00:40,  1.06it/s]38it [00:40,  1.05it/s]30it [00:40,  1.06s/it]36it [00:40,  1.26it/s]36it [00:40,  1.23s/it]40it [00:40,  1.09it/s]31it [00:41,  1.19it/s]37it [00:41,  1.05it/s]30it [00:41,  1.33s/it]41it [00:41,  1.35it/s]39it [00:41,  1.02it/s]31it [00:42,  1.12s/it]29it [00:42,  2.04s/it]36it [00:42,  1.39s/it]40it [00:42,  1.12it/s]37it [00:42,  1.05s/it]32it [00:42,  1.00it/s]38it [00:42,  1.17s/it]41it [00:42,  1.32it/s]30it [00:43,  1.67s/it]38it [00:42,  1.15it/s]42it [00:43,  1.04s/it]39it [00:43,  1.11it/s]43it [00:43,  1.22it/s]32it [00:43,  1.28s/it]39it [00:43,  1.28it/s]40it [00:43,  1.29it/s]44it [00:43,  1.36it/s]42it [00:44,  1.12it/s]31it [00:44,  1.59s/it]33it [00:44,  1.26s/it]41it [00:44,  1.29it/s]45it [00:44,  1.38it/s]40it [00:44,  1.12it/s]42it [00:44,  1.56it/s]34it [00:45,  1.01s/it]41it [00:45,  1.39it/s]37it [00:44,  1.81s/it]43it [00:45,  1.07it/s]35it [00:45,  1.14it/s]44it [00:45,  1.32it/s]33it [00:45,  1.55s/it]32it [00:45,  1.52s/it]45it [00:45,  1.57it/s]42it [00:45,  1.33it/s]46it [00:45,  1.11it/s]34it [00:46,  1.24s/it]47it [00:46,  1.38it/s]43it [00:46,  1.11it/s]38it [00:46,  1.70s/it]48it [00:46,  1.52it/s]39it [00:46,  1.35s/it]36it [00:47,  1.16s/it]46it [00:47,  1.14it/s]44it [00:47,  1.05it/s]33it [00:47,  1.54s/it]40it [00:47,  1.05s/it]49it [00:47,  1.40it/s]37it [00:47,  1.06it/s]45it [00:47,  1.19it/s]35it [00:47,  1.39s/it]38it [00:48,  1.23it/s]50it [00:48,  1.44it/s]46it [00:48,  1.46it/s]47it [00:48,  1.02it/s]39it [00:49,  1.38it/s]43it [00:48,  1.38s/it]36it [00:49,  1.36s/it]41it [00:49,  1.29s/it]40it [00:49,  1.49it/s]34it [00:49,  1.66s/it]48it [00:49,  1.04it/s]47it [00:49,  1.12it/s]44it [00:49,  1.27s/it]41it [00:50,  1.57it/s]35it [00:49,  1.34s/it]51it [00:49,  1.02s/it]52it [00:50,  1.15it/s]37it [00:50,  1.37s/it]36it [00:50,  1.18s/it]42it [00:51,  1.40it/s]49it [00:50,  1.11s/it]48it [00:51,  1.07s/it]42it [00:51,  1.51s/it]45it [00:51,  1.40s/it]43it [00:51,  1.28s/it]37it [00:52,  1.24s/it]43it [00:52,  1.03s/it]50it [00:52,  1.27s/it]44it [00:52,  1.09s/it]46it [00:52,  1.35s/it]53it [00:52,  1.28s/it]38it [00:52,  1.68s/it]47it [00:53,  1.14s/it]49it [00:53,  1.43s/it]45it [00:53,  1.02s/it]54it [00:53,  1.14s/it]51it [00:53,  1.20s/it]50it [00:53,  1.12s/it]46it [00:53,  1.23it/s]52it [00:54,  1.02it/s]47it [00:54,  1.46it/s]51it [00:54,  1.04it/s]53it [00:54,  1.25it/s]38it [00:54,  1.62s/it]52it [00:54,  1.31it/s]39it [00:54,  1.72s/it]39it [00:54,  1.25s/it]54it [00:55,  1.27it/s]48it [00:55,  1.36s/it]44it [00:55,  1.57s/it]55it [00:55,  1.36s/it]40it [00:55,  1.04s/it]48it [00:56,  1.11s/it]53it [00:56,  1.03s/it]40it [00:56,  1.71s/it]49it [00:56,  1.39s/it]45it [00:57,  1.66s/it]56it [00:57,  1.48s/it]57it [00:57,  1.20s/it]41it [00:57,  1.43s/it]54it [00:57,  1.18s/it]55it [00:58,  1.43s/it]55it [00:58,  1.06it/s]49it [00:58,  1.37s/it]41it [00:58,  1.80s/it]56it [00:58,  1.30it/s]57it [00:59,  1.42it/s]46it [00:59,  1.78s/it]58it [00:59,  1.29s/it]50it [00:59,  1.77s/it]50it [00:59,  1.30s/it]51it [00:59,  1.36s/it]51it [00:59,  1.04s/it]59it [00:59,  1.12s/it]58it [00:59,  1.37it/s]42it [00:59,  1.70s/it]42it [01:00,  1.67s/it]52it [01:00,  1.22it/s]60it [01:00,  1.14it/s]43it [01:00,  1.29s/it]61it [01:00,  1.39it/s]43it [01:00,  1.39s/it]44it [01:00,  1.00s/it]47it [01:01,  1.75s/it]52it [01:01,  1.41s/it]56it [01:01,  1.97s/it]59it [01:01,  1.08it/s]45it [01:01,  1.15it/s]48it [01:01,  1.39s/it]53it [01:01,  1.18s/it]62it [01:01,  1.13it/s]46it [01:02,  1.26it/s]53it [01:01,  1.13s/it]54it [01:02,  1.07it/s]57it [01:02,  1.77s/it]47it [01:02,  1.26it/s]55it [01:02,  1.22it/s]49it [01:03,  1.42s/it]56it [01:03,  1.45it/s]44it [01:03,  1.81s/it]50it [01:03,  1.16s/it]48it [01:03,  1.25it/s]57it [01:03,  1.50it/s]60it [01:03,  1.42s/it]49it [01:04,  1.41it/s]58it [01:04,  1.65it/s]58it [01:04,  1.73s/it]45it [01:04,  1.56s/it]63it [01:04,  1.39s/it]59it [01:04,  1.79it/s]50it [01:04,  1.43it/s]54it [01:04,  1.61s/it]46it [01:04,  1.19s/it]64it [01:04,  1.06s/it]61it [01:04,  1.25s/it]51it [01:05,  1.27s/it]65it [01:05,  1.19it/s]62it [01:05,  1.01it/s]47it [01:05,  1.01s/it]51it [01:05,  1.41it/s]60it [01:05,  1.59it/s]52it [01:05,  1.08s/it]66it [01:06,  1.13it/s]61it [01:06,  1.58it/s]63it [01:06,  1.00s/it]55it [01:06,  1.65s/it]53it [01:07,  1.07s/it]67it [01:06,  1.20it/s]59it [01:07,  2.03s/it]54it [01:07,  1.15it/s]48it [01:07,  1.25s/it]68it [01:07,  1.33it/s]60it [01:07,  1.54s/it]49it [01:07,  1.03it/s]52it [01:07,  1.12s/it]56it [01:07,  1.52s/it]69it [01:07,  1.44it/s]61it [01:07,  1.24s/it]53it [01:08,  1.05it/s]57it [01:08,  1.21s/it]58it [01:08,  1.06it/s]70it [01:08,  1.36it/s]50it [01:08,  1.13s/it]62it [01:09,  1.19s/it]59it [01:09,  1.20it/s]62it [01:09,  1.37s/it]71it [01:09,  1.41it/s]55it [01:09,  1.36s/it]51it [01:09,  1.00s/it]64it [01:09,  1.75s/it]60it [01:09,  1.28it/s]63it [01:10,  1.22s/it]72it [01:10,  1.41it/s]64it [01:10,  1.00it/s]73it [01:10,  1.51it/s]54it [01:10,  1.46s/it]52it [01:11,  1.17s/it]63it [01:11,  1.57s/it]61it [01:11,  1.09s/it]65it [01:11,  1.07s/it]64it [01:12,  1.28s/it]62it [01:12,  1.07it/s]55it [01:12,  1.44s/it]66it [01:12,  1.10it/s]56it [01:12,  1.80s/it]63it [01:12,  1.30it/s]53it [01:12,  1.21s/it]65it [01:12,  2.13s/it]64it [01:12,  1.59it/s]67it [01:12,  1.26it/s]56it [01:13,  1.32s/it]54it [01:13,  1.10s/it]74it [01:13,  1.29s/it]65it [01:13,  1.45it/s]65it [01:13,  1.38s/it]75it [01:13,  1.02s/it]57it [01:14,  1.72s/it]57it [01:14,  1.24s/it]66it [01:14,  1.14s/it]55it [01:14,  1.06s/it]76it [01:14,  1.14it/s]66it [01:14,  1.38it/s]67it [01:14,  1.11it/s]66it [01:14,  2.17s/it]68it [01:14,  1.20s/it]68it [01:15,  1.20it/s]69it [01:15,  1.01s/it]58it [01:16,  1.77s/it]67it [01:16,  1.07s/it]59it [01:16,  1.39s/it]70it [01:16,  1.05it/s]67it [01:16,  1.98s/it]69it [01:16,  1.04s/it]77it [01:16,  1.37s/it]68it [01:17,  1.55s/it]56it [01:17,  1.58s/it]78it [01:17,  1.05s/it]60it [01:17,  1.21s/it]58it [01:17,  1.75s/it]70it [01:17,  1.16it/s]61it [01:17,  1.03it/s]68it [01:17,  1.18s/it]62it [01:18,  1.18it/s]59it [01:18,  1.52s/it]69it [01:18,  1.01s/it]71it [01:18,  1.27s/it]60it [01:19,  1.35s/it]71it [01:19,  1.23s/it]57it [01:19,  1.77s/it]70it [01:19,  1.03s/it]72it [01:19,  1.31s/it]71it [01:20,  1.09it/s]79it [01:20,  1.63s/it]73it [01:20,  1.04s/it]63it [01:20,  1.26s/it]72it [01:20,  1.18s/it]80it [01:20,  1.33s/it]74it [01:21,  1.02it/s]58it [01:21,  1.76s/it]61it [01:21,  1.54s/it]72it [01:21,  1.06it/s]64it [01:21,  1.12s/it]73it [01:21,  1.20s/it]69it [01:21,  2.52s/it]81it [01:22,  1.30s/it]59it [01:22,  1.57s/it]62it [01:22,  1.43s/it]70it [01:22,  1.95s/it]73it [01:22,  1.08s/it]63it [01:22,  1.18s/it]71it [01:23,  1.54s/it]75it [01:23,  1.30s/it]74it [01:23,  1.31s/it]76it [01:23,  1.02s/it]64it [01:23,  1.02s/it]65it [01:23,  1.51s/it]82it [01:23,  1.39s/it]77it [01:23,  1.18it/s]72it [01:23,  1.35s/it]66it [01:24,  1.18s/it]74it [01:24,  1.23s/it]78it [01:24,  1.30it/s]75it [01:24,  1.32s/it]83it [01:24,  1.32s/it]73it [01:24,  1.24s/it]65it [01:25,  1.27s/it]74it [01:25,  1.04s/it]84it [01:25,  1.17s/it]76it [01:26,  1.36s/it]67it [01:26,  1.51s/it]60it [01:26,  2.32s/it]79it [01:26,  1.25s/it]68it [01:27,  1.27s/it]60it [01:27,  1.45s/it]
[rank7]: Traceback (most recent call last):
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls4.py", line 569, in <module>
[rank7]:     evaluate_chat_model(model, processor, image_process_args, args.select_mode, args.select_image_num)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls4.py", line 386, in evaluate_chat_model
[rank7]:     page_pred, page_drop = predict_page(inputs[0].to(model.device),
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls4.py", line 220, in predict_page
[rank7]:     image_embeds = qwen2vl_generation_model.visual(pixel_values, grid_thw=image_grid_thw).to(inputs_embeds.device)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 1197, in forward
[rank7]:     hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 432, in forward
[rank7]:     hidden_states = hidden_states + self.attn(
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 373, in forward
[rank7]:     q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 256, in apply_rotary_pos_emb_vision
[rank7]:     output = (tensor * cos) + (rotate_half(tensor) * sin)
[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 230.00 MiB. GPU 7 has a total capacity of 79.35 GiB of which 200.19 MiB is free. Process 130037 has 79.15 GiB memory in use. Of the allocated memory 74.99 GiB is allocated by PyTorch, and 3.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
80it [01:27,  1.03it/s]66it [01:27,  1.48s/it]69it [01:27,  1.01s/it]77it [01:27,  1.41s/it]75it [01:27,  1.34s/it]81it [01:27,  1.19it/s]85it [01:27,  1.49s/it]75it [01:27,  2.00s/it]70it [01:28,  1.13it/s]67it [01:28,  1.31s/it]W1216 17:59:11.219000 140107021306752 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177604 closing signal SIGTERM
W1216 17:59:11.219000 140107021306752 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177605 closing signal SIGTERM
W1216 17:59:11.219000 140107021306752 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177606 closing signal SIGTERM
W1216 17:59:11.219000 140107021306752 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177607 closing signal SIGTERM
W1216 17:59:11.220000 140107021306752 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177608 closing signal SIGTERM
W1216 17:59:11.220000 140107021306752 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177609 closing signal SIGTERM
W1216 17:59:11.220000 140107021306752 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177610 closing signal SIGTERM
E1216 17:59:13.383000 140107021306752 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 7 (pid: 177611) of binary: /root/miniconda3/bin/python
Traceback (most recent call last):
  File "/root/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/root/miniconda3/lib/python3.10/site-packages/zhenjin_utils/import_hooks/torchrun_args_hook.py", line 28, in new_run
    return module.old_run(*func_args, **func_kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
infer_mpdocvqa_dynamic_unsupervised_cls4.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-16_17:59:11
  host      : kmaker-kmaker-033145099100
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 177611)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-newsvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-newsvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-newsvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-newsvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-newsvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-slidevqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-slidevqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-slidevqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-slidevqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix == vqa-slidevqa-val-unsupervised-cls-v4-top5_mix ']'
