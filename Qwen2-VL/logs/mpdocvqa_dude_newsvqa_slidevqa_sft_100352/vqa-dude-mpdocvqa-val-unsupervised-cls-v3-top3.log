+ CHECKPOINT=/gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
+ DATASET=vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3
+ GPUS=8
+ CHECKPOINT=/gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
++ pwd
+ export PYTHONPATH=/mntnlp/zqp/physical_report/Qwen2-VL:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public
+ PYTHONPATH=/mntnlp/zqp/physical_report/Qwen2-VL:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public
+ echo 'CHECKPOINT: /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/'
CHECKPOINT: /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
+ MASTER_PORT=62621
+ PORT=62621
+ GPUS=8
+ export MASTER_PORT=62621
+ MASTER_PORT=62621
+ export PORT=62621
+ PORT=62621
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-mpdocvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-mpdocvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-mpdocvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-mpdocvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-mpdocvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-dude-mpdocvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 ']'
+ torchrun --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --nproc_per_node=8 --master_port=62621 infer_mpdocvqa_dynamic_unsupervised_cls3.py --checkpoint /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/ --datasets dude_mpdocvqa_val --max-pixels-low 200704 --max-pixels-high 802816 --out-dir ./res/dude-mpdoc-vqa --dynamic --select-mode topk --select-image-num 3
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.53it/s]Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  5.05it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.02it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  5.15it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.63it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.07it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.52it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.04it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.05it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.74it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.03it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.35it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  3.91it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.63it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.74it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.59it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.57it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.22it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  3.98it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.80it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.88it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.73it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.74it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.71it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  3.88it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.34it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.19it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.88it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.76it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.82it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.70it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.78it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.68it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.85it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.84it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.30it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.12it/s]
0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]1it [00:01,  1.28s/it]1it [00:01,  1.60s/it]2it [00:02,  1.07s/it]1it [00:02,  2.50s/it]2it [00:02,  1.30s/it]3it [00:02,  1.18it/s]2it [00:02,  1.29s/it]1it [00:02,  2.06s/it]1it [00:02,  2.13s/it]2it [00:02,  1.04s/it]1it [00:03,  3.24s/it]4it [00:03,  1.46it/s]1it [00:02,  2.75s/it]5it [00:03,  1.68it/s]2it [00:03,  1.59s/it]3it [00:04,  1.19s/it]1it [00:04,  4.23s/it]2it [00:03,  1.50s/it]3it [00:04,  1.12s/it]2it [00:03,  1.61s/it]3it [00:04,  1.46s/it]3it [00:03,  1.01s/it]4it [00:04,  1.11it/s]4it [00:04,  1.09s/it]2it [00:05,  2.36s/it]3it [00:04,  1.30s/it]4it [00:04,  1.10it/s]3it [00:04,  1.63s/it]5it [00:05,  1.05s/it]4it [00:04,  1.04s/it]3it [00:06,  1.65s/it]4it [00:05,  1.14s/it]4it [00:06,  1.60s/it]5it [00:05,  1.15it/s]6it [00:06,  1.16it/s]5it [00:06,  1.19s/it]4it [00:07,  1.38s/it]5it [00:06,  1.09s/it]6it [00:07,  1.03it/s]7it [00:07,  1.12it/s]5it [00:06,  1.25s/it]6it [00:07,  1.67s/it]7it [00:07,  1.24s/it]6it [00:07,  1.01it/s]8it [00:07,  1.26it/s]9it [00:08,  1.44it/s]5it [00:08,  1.56s/it]6it [00:07,  1.30s/it]6it [00:08,  1.58s/it]7it [00:09,  1.38s/it]8it [00:09,  1.04s/it]7it [00:08,  1.23s/it]5it [00:09,  2.32s/it]7it [00:09,  1.33s/it]9it [00:09,  1.21it/s]6it [00:10,  1.47s/it]7it [00:09,  1.31s/it]8it [00:10,  1.65s/it]8it [00:09,  1.07s/it]8it [00:09,  1.08s/it]7it [00:10,  1.21s/it]9it [00:09,  1.20it/s]10it [00:10,  1.17it/s]8it [00:11,  1.08it/s]6it [00:11,  2.00s/it]10it [00:11,  1.39s/it]9it [00:10,  1.05s/it]8it [00:10,  1.33s/it]9it [00:11,  1.23it/s]9it [00:11,  1.56s/it]9it [00:10,  1.04s/it]10it [00:12,  1.49it/s]11it [00:12,  1.02it/s]10it [00:11,  1.01it/s]7it [00:12,  1.73s/it]11it [00:12,  1.68it/s]11it [00:12,  1.32s/it]10it [00:11,  1.13s/it]8it [00:12,  1.36s/it]10it [00:13,  1.46s/it]11it [00:12,  1.05it/s]12it [00:13,  1.00it/s]12it [00:13,  1.18s/it]13it [00:13,  1.27it/s]12it [00:13,  1.20it/s]10it [00:12,  1.35s/it]11it [00:13,  1.31s/it]12it [00:13,  1.02it/s]13it [00:14,  1.42it/s]11it [00:13,  1.28s/it]13it [00:14,  1.22s/it]13it [00:13,  1.14it/s]12it [00:14,  1.09s/it]9it [00:14,  1.56s/it]14it [00:15,  1.06it/s]12it [00:15,  1.24s/it]14it [00:14,  1.42it/s]14it [00:15,  1.08s/it]13it [00:14,  1.08it/s]15it [00:14,  1.62it/s]11it [00:14,  1.42s/it]14it [00:14,  1.35it/s]13it [00:15,  1.14s/it]15it [00:15,  1.04it/s]15it [00:15,  1.58it/s]10it [00:15,  1.45s/it]15it [00:16,  1.04s/it]16it [00:16,  1.13it/s]14it [00:16,  1.04s/it]14it [00:16,  1.29s/it]16it [00:16,  1.49it/s]12it [00:16,  1.43s/it]11it [00:16,  1.26s/it]17it [00:16,  1.80it/s]15it [00:17,  1.03s/it]17it [00:17,  1.23it/s]16it [00:17,  1.06s/it]15it [00:17,  1.07it/s]12it [00:17,  1.07s/it]18it [00:17,  1.41it/s]18it [00:17,  1.65it/s]16it [00:18,  1.21it/s]13it [00:17,  1.14it/s]13it [00:17,  1.35s/it]19it [00:18,  1.68it/s]19it [00:17,  1.77it/s]16it [00:17,  1.32s/it]14it [00:18,  1.11it/s]20it [00:18,  1.47it/s]14it [00:18,  1.35s/it]16it [00:19,  1.41s/it]17it [00:20,  1.09s/it]20it [00:19,  1.17it/s]15it [00:18,  1.08s/it]17it [00:19,  1.53s/it]21it [00:20,  1.21it/s]17it [00:19,  1.46s/it]17it [00:20,  1.24s/it]16it [00:19,  1.18it/s]21it [00:19,  1.41it/s]18it [00:19,  1.13s/it]15it [00:20,  1.13s/it]18it [00:20,  1.33s/it]17it [00:20,  1.13it/s]22it [00:20,  1.25it/s]22it [00:21,  1.06it/s]19it [00:21,  1.10s/it]16it [00:21,  1.02s/it]18it [00:20,  1.39it/s]18it [00:22,  1.42s/it]23it [00:21,  1.11it/s]17it [00:22,  1.02s/it]18it [00:22,  1.55s/it]23it [00:22,  1.02s/it]20it [00:22,  1.14s/it]19it [00:21,  1.14it/s]24it [00:22,  1.27it/s]19it [00:23,  1.34s/it]24it [00:23,  1.07it/s]21it [00:23,  1.02s/it]20it [00:22,  1.26it/s]18it [00:23,  1.06s/it]25it [00:22,  1.31it/s]25it [00:23,  1.29it/s]19it [00:22,  1.74s/it]20it [00:23,  1.10s/it]22it [00:24,  1.04s/it]19it [00:24,  1.69s/it]21it [00:24,  1.05s/it]23it [00:24,  1.19it/s]22it [00:24,  1.22it/s]20it [00:25,  1.35s/it]26it [00:24,  1.04it/s]21it [00:24,  1.08s/it]19it [00:25,  1.24s/it]24it [00:25,  1.34it/s]23it [00:25,  1.22it/s]25it [00:25,  1.49it/s]21it [00:26,  1.29s/it]20it [00:25,  1.99s/it]24it [00:26,  1.31it/s]27it [00:25,  1.09s/it]22it [00:26,  1.01s/it]25it [00:26,  1.51it/s]22it [00:25,  1.28s/it]20it [00:26,  1.38s/it]21it [00:27,  1.11s/it]23it [00:27,  1.00s/it]26it [00:27,  1.69s/it]21it [00:26,  1.81s/it]28it [00:27,  1.19s/it]22it [00:27,  1.39s/it]24it [00:28,  1.10it/s]22it [00:28,  1.03s/it]25it [00:28,  1.35it/s]26it [00:28,  1.31s/it]23it [00:28,  1.20it/s]27it [00:28,  1.53s/it]27it [00:29,  1.02s/it]29it [00:28,  1.18s/it]28it [00:28,  1.18s/it]26it [00:29,  1.44it/s]26it [00:29,  1.17s/it]30it [00:28,  1.09it/s]23it [00:28,  1.66s/it]24it [00:29,  1.18it/s]25it [00:29,  1.37it/s]29it [00:30,  1.16s/it]27it [00:30,  1.13s/it]23it [00:29,  1.65s/it]28it [00:30,  1.11it/s]24it [00:29,  1.62s/it]30it [00:30,  1.05s/it]27it [00:31,  1.06s/it]31it [00:30,  1.17s/it]28it [00:31,  1.36s/it]25it [00:30,  1.30s/it]28it [00:31,  1.13it/s]24it [00:30,  1.59s/it]29it [00:31,  1.19s/it]29it [00:32,  1.36it/s]25it [00:31,  1.22s/it]29it [00:32,  1.16s/it]26it [00:32,  1.23s/it]30it [00:32,  1.01s/it]31it [00:32,  1.26s/it]26it [00:31,  1.27s/it]31it [00:32,  1.26it/s]32it [00:31,  1.33s/it]30it [00:33,  1.28it/s]32it [00:33,  1.53it/s]33it [00:32,  1.11s/it]31it [00:33,  1.36it/s]32it [00:33,  1.14s/it]27it [00:32,  1.21s/it]30it [00:33,  1.26s/it]33it [00:33,  1.11it/s]27it [00:33,  1.34s/it]26it [00:33,  1.41s/it]33it [00:34,  1.33it/s]34it [00:33,  1.01s/it]32it [00:34,  1.33it/s]31it [00:34,  1.05s/it]34it [00:34,  1.23it/s]28it [00:33,  1.08s/it]35it [00:33,  1.22it/s]33it [00:34,  1.60it/s]29it [00:33,  1.17it/s]27it [00:34,  1.29s/it]34it [00:35,  1.62it/s]36it [00:34,  1.30it/s]32it [00:35,  1.01it/s]34it [00:35,  1.09it/s]28it [00:34,  1.06s/it]35it [00:35,  1.11it/s]35it [00:35,  1.34it/s]33it [00:36,  1.11it/s]30it [00:35,  1.07it/s]36it [00:36,  1.20it/s]31it [00:35,  1.34it/s]35it [00:36,  1.25it/s]34it [00:36,  1.32it/s]36it [00:36,  1.33it/s]28it [00:36,  1.73s/it]37it [00:36,  1.05s/it]29it [00:36,  1.16s/it]35it [00:37,  1.33it/s]37it [00:37,  1.10it/s]38it [00:36,  1.07it/s]37it [00:37,  1.10it/s]36it [00:38,  1.07s/it]38it [00:38,  1.14it/s]38it [00:38,  1.31it/s]30it [00:37,  1.20s/it]39it [00:37,  1.15it/s]32it [00:37,  1.13s/it]37it [00:38,  1.20it/s]29it [00:38,  1.82s/it]39it [00:38,  1.43it/s]31it [00:37,  1.02s/it]39it [00:39,  1.12it/s]40it [00:38,  1.15it/s]30it [00:39,  1.52s/it]41it [00:38,  1.42it/s]32it [00:38,  1.06it/s]38it [00:39,  1.03it/s]36it [00:39,  1.31s/it]40it [00:39,  1.21it/s]40it [00:39,  1.22it/s]39it [00:40,  1.31it/s]41it [00:40,  1.49it/s]41it [00:40,  1.42it/s]33it [00:39,  1.36s/it]40it [00:40,  1.47it/s]31it [00:40,  1.39s/it]34it [00:39,  1.11s/it]42it [00:40,  1.09it/s]42it [00:41,  1.40it/s]33it [00:40,  1.13s/it]43it [00:40,  1.36it/s]42it [00:41,  1.25it/s]41it [00:41,  1.39it/s]42it [00:41,  1.66it/s]34it [00:40,  1.08it/s]32it [00:41,  1.30s/it]44it [00:40,  1.48it/s]43it [00:42,  1.22it/s]35it [00:41,  1.23it/s]37it [00:42,  1.68s/it]45it [00:41,  1.46it/s]44it [00:42,  1.47it/s]35it [00:41,  1.31s/it]45it [00:42,  1.74it/s]43it [00:43,  1.20it/s]33it [00:42,  1.33s/it]46it [00:42,  1.28it/s]38it [00:43,  1.53s/it]36it [00:42,  1.21s/it]43it [00:43,  1.28s/it]36it [00:42,  1.04s/it]47it [00:42,  1.56it/s]44it [00:44,  1.17it/s]46it [00:43,  1.34it/s]39it [00:44,  1.24s/it]37it [00:43,  1.17it/s]48it [00:43,  1.65it/s]40it [00:44,  1.03it/s]45it [00:44,  1.29it/s]37it [00:43,  1.18s/it]44it [00:44,  1.20s/it]38it [00:43,  1.33it/s]34it [00:44,  1.43s/it]46it [00:44,  1.57it/s]47it [00:44,  1.18it/s]49it [00:44,  1.48it/s]39it [00:44,  1.48it/s]35it [00:45,  1.18s/it]40it [00:44,  1.57it/s]50it [00:44,  1.51it/s]47it [00:46,  1.28it/s]48it [00:45,  1.16it/s]41it [00:45,  1.15s/it]36it [00:45,  1.06s/it]45it [00:46,  1.30s/it]41it [00:45,  1.63it/s]38it [00:45,  1.45s/it]49it [00:47,  1.04it/s]42it [00:46,  1.45it/s]37it [00:47,  1.07s/it]48it [00:47,  1.07it/s]51it [00:46,  1.08it/s]46it [00:47,  1.25s/it]42it [00:47,  1.32s/it]52it [00:46,  1.26it/s]47it [00:47,  1.07s/it]39it [00:47,  1.48s/it]43it [00:48,  1.13s/it]50it [00:48,  1.08s/it]43it [00:47,  1.09it/s]44it [00:49,  1.01it/s]49it [00:49,  1.24s/it]51it [00:49,  1.02s/it]38it [00:49,  1.40s/it]48it [00:49,  1.22s/it]50it [00:49,  1.01it/s]53it [00:48,  1.12s/it]39it [00:49,  1.08s/it]40it [00:48,  1.47s/it]52it [00:49,  1.18it/s]45it [00:49,  1.06it/s]51it [00:50,  1.14it/s]53it [00:50,  1.41it/s]46it [00:50,  1.31it/s]40it [00:50,  1.08it/s]52it [00:50,  1.41it/s]54it [00:49,  1.04s/it]47it [00:50,  1.54it/s]49it [00:50,  1.25s/it]54it [00:50,  1.39it/s]44it [00:50,  1.40s/it]41it [00:50,  1.61s/it]53it [00:52,  1.09it/s]55it [00:51,  1.20s/it]41it [00:52,  1.26s/it]48it [00:52,  1.01s/it]45it [00:51,  1.45s/it]42it [00:51,  1.49s/it]54it [00:53,  1.01s/it]50it [00:53,  1.58s/it]55it [00:53,  1.24s/it]55it [00:53,  1.21it/s]43it [00:52,  1.25s/it]51it [00:53,  1.22s/it]56it [00:52,  1.31s/it]56it [00:54,  1.45it/s]49it [00:54,  1.22s/it]57it [00:53,  1.08s/it]57it [00:54,  1.54it/s]46it [00:53,  1.55s/it]42it [00:54,  1.53s/it]52it [00:54,  1.23s/it]43it [00:54,  1.19s/it]50it [00:55,  1.15s/it]44it [00:54,  1.08it/s]58it [00:55,  1.44it/s]58it [00:54,  1.12s/it]53it [00:55,  1.04s/it]51it [00:55,  1.07it/s]45it [00:55,  1.24it/s]54it [00:55,  1.21it/s]47it [00:54,  1.50s/it]52it [00:55,  1.35it/s]44it [00:55,  1.60s/it]59it [00:55,  1.01it/s]56it [00:56,  1.73s/it]46it [00:56,  1.36it/s]59it [00:56,  1.25it/s]55it [00:56,  1.35it/s]48it [00:55,  1.21s/it]60it [00:55,  1.26it/s]56it [00:56,  1.57it/s]61it [00:55,  1.51it/s]45it [00:55,  1.40s/it]47it [00:56,  1.31it/s]46it [00:56,  1.08s/it]57it [00:57,  1.52s/it]57it [00:57,  1.60it/s]53it [00:57,  1.01s/it]49it [00:56,  1.19s/it]58it [00:57,  1.74it/s]62it [00:56,  1.32it/s]47it [00:56,  1.06it/s]48it [00:57,  1.28it/s]50it [00:57,  1.01s/it]59it [00:58,  1.84it/s]49it [00:58,  1.43it/s]60it [00:58,  1.25s/it]58it [00:58,  1.46s/it]60it [00:59,  1.62it/s]50it [00:58,  1.47it/s]48it [00:58,  1.10s/it]51it [00:58,  1.09s/it]61it [00:59,  1.15s/it]61it [00:59,  1.63it/s]49it [00:58,  1.16it/s]51it [00:59,  1.43it/s]62it [01:00,  1.09it/s]54it [00:59,  1.45s/it]52it [00:59,  1.05it/s]63it [00:59,  1.22s/it]64it [00:59,  1.06it/s]65it [00:59,  1.32it/s]53it [01:00,  1.07it/s]63it [01:01,  1.05it/s]50it [01:00,  1.07s/it]54it [01:00,  1.28it/s]59it [01:01,  1.87s/it]55it [01:01,  1.45s/it]52it [01:01,  1.03s/it]66it [01:00,  1.21it/s]60it [01:01,  1.45s/it]51it [01:00,  1.03it/s]53it [01:01,  1.13it/s]62it [01:02,  1.24s/it]56it [01:02,  1.30s/it]67it [01:01,  1.26it/s]61it [01:02,  1.18s/it]57it [01:02,  1.06s/it]68it [01:02,  1.38it/s]63it [01:03,  1.13s/it]52it [01:02,  1.07s/it]58it [01:03,  1.18it/s]62it [01:03,  1.09s/it]69it [01:02,  1.48it/s]55it [01:02,  1.22s/it]64it [01:03,  1.08it/s]64it [01:03,  1.54s/it]59it [01:03,  1.30it/s]70it [01:03,  1.39it/s]53it [01:03,  1.08s/it]60it [01:04,  1.36it/s]54it [01:04,  1.34s/it]65it [01:04,  1.06it/s]71it [01:04,  1.42it/s]54it [01:04,  1.00it/s]66it [01:05,  1.22it/s]63it [01:05,  1.41s/it]67it [01:05,  1.36it/s]72it [01:04,  1.42it/s]55it [01:05,  1.36s/it]61it [01:05,  1.03it/s]64it [01:06,  1.17s/it]56it [01:05,  1.61s/it]55it [01:05,  1.01it/s]73it [01:05,  1.52it/s]65it [01:06,  1.83s/it]62it [01:06,  1.19it/s]63it [01:06,  1.42it/s]56it [01:06,  1.26s/it]64it [01:07,  1.71it/s]65it [01:07,  1.20s/it]57it [01:06,  1.51s/it]68it [01:07,  1.05s/it]57it [01:07,  1.16s/it]66it [01:07,  1.02s/it]65it [01:08,  1.54it/s]69it [01:08,  1.11it/s]67it [01:08,  1.23it/s]56it [01:07,  1.43s/it]66it [01:08,  1.44it/s]66it [01:09,  2.04s/it]70it [01:08,  1.14it/s]74it [01:08,  1.25s/it]68it [01:08,  1.32it/s]58it [01:08,  1.53s/it]75it [01:08,  1.01it/s]59it [01:08,  1.22s/it]76it [01:09,  1.14it/s]67it [01:10,  1.80s/it]69it [01:10,  1.12it/s]60it [01:09,  1.10s/it]67it [01:10,  1.03it/s]57it [01:09,  1.57s/it]70it [01:10,  1.33it/s]71it [01:10,  1.14s/it]68it [01:10,  1.43s/it]58it [01:10,  1.63s/it]61it [01:09,  1.12it/s]62it [01:10,  1.25it/s]59it [01:11,  1.40s/it]68it [01:11,  1.03s/it]72it [01:11,  1.13s/it]58it [01:10,  1.52s/it]73it [01:12,  1.09it/s]77it [01:11,  1.29s/it]69it [01:12,  1.10it/s]60it [01:12,  1.22s/it]71it [01:12,  1.07s/it]78it [01:11,  1.00s/it]69it [01:13,  1.68s/it]59it [01:12,  1.39s/it]74it [01:13,  1.12it/s]70it [01:13,  1.10it/s]72it [01:13,  1.03s/it]63it [01:12,  1.21s/it]70it [01:13,  1.37s/it]71it [01:13,  1.19it/s]61it [01:13,  1.39s/it]71it [01:14,  1.13s/it]64it [01:13,  1.08s/it]73it [01:14,  1.05s/it]75it [01:14,  1.14s/it]72it [01:14,  1.13it/s]72it [01:15,  1.04s/it]76it [01:15,  1.10it/s]62it [01:14,  1.28s/it]79it [01:14,  1.50s/it]77it [01:15,  1.29it/s]63it [01:15,  1.08s/it]80it [01:14,  1.24s/it]74it [01:15,  1.18s/it]73it [01:15,  1.05it/s]73it [01:16,  1.02s/it]60it [01:15,  1.88s/it]78it [01:16,  1.41it/s]65it [01:15,  1.39s/it]64it [01:16,  1.05it/s]74it [01:16,  1.13it/s]66it [01:15,  1.10s/it]60it [01:15,  1.26s/it]
75it [01:16,  1.13s/it][rank7]: Traceback (most recent call last):
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls3.py", line 550, in <module>
[rank7]:     evaluate_chat_model(model, processor, image_process_args, args.select_mode, args.select_image_num)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls3.py", line 367, in evaluate_chat_model
[rank7]:     page_pred = predict_page(inputs[0].to(model.device),
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls3.py", line 220, in predict_page
[rank7]:     image_embeds = qwen2vl_generation_model.visual(pixel_values, grid_thw=image_grid_thw).to(inputs_embeds.device)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 1197, in forward
[rank7]:     hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 435, in forward
[rank7]:     hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 323, in forward
[rank7]:     return self.fc2(self.act(self.fc1(x)))
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/transformers/activations.py", line 96, in forward
[rank7]:     return input * torch.sigmoid(1.702 * input)
[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 7 has a total capacity of 79.35 GiB of which 24.19 MiB is free. Process 460459 has 79.32 GiB memory in use. Of the allocated memory 75.36 GiB is allocated by PyTorch, and 3.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
81it [01:16,  1.20s/it]74it [01:17,  1.00s/it]65it [01:17,  1.14s/it]79it [01:18,  1.10s/it]82it [01:17,  1.22s/it]76it [01:18,  1.20s/it]75it [01:18,  1.14s/it]80it [01:18,  1.14it/s]W1216 17:52:48.491000 139943375858560 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 173719 closing signal SIGTERM
W1216 17:52:48.491000 139943375858560 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 173720 closing signal SIGTERM
W1216 17:52:48.492000 139943375858560 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 173721 closing signal SIGTERM
W1216 17:52:48.494000 139943375858560 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 173722 closing signal SIGTERM
W1216 17:52:48.495000 139943375858560 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 173723 closing signal SIGTERM
W1216 17:52:48.496000 139943375858560 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 173724 closing signal SIGTERM
W1216 17:52:48.497000 139943375858560 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 173725 closing signal SIGTERM
E1216 17:52:50.506000 139943375858560 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 7 (pid: 173726) of binary: /root/miniconda3/bin/python
Traceback (most recent call last):
  File "/root/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/root/miniconda3/lib/python3.10/site-packages/zhenjin_utils/import_hooks/torchrun_args_hook.py", line 28, in new_run
    return module.old_run(*func_args, **func_kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
infer_mpdocvqa_dynamic_unsupervised_cls3.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-16_17:52:48
  host      : kmaker-kmaker-033145099100
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 173726)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-newsvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-newsvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-newsvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-newsvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-newsvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-slidevqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-slidevqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-slidevqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-slidevqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 == vqa-slidevqa-val-unsupervised-cls-v4-top5_mix ']'
