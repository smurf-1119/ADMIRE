+ CHECKPOINT=/gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
+ DATASET=vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5
+ GPUS=8
+ CHECKPOINT=/gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
++ pwd
+ export PYTHONPATH=/mntnlp/zqp/physical_report/Qwen2-VL:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public
+ PYTHONPATH=/mntnlp/zqp/physical_report/Qwen2-VL:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public:/root/miniconda3/lib/python3.10/site-packages/aistudio_notebook/public
+ echo 'CHECKPOINT: /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/'
CHECKPOINT: /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/
+ MASTER_PORT=62621
+ PORT=62621
+ GPUS=8
+ export MASTER_PORT=62621
+ MASTER_PORT=62621
+ export PORT=62621
+ PORT=62621
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-mpdocvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-mpdocvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-mpdocvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-mpdocvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-mpdocvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-dude-mpdocvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 ']'
+ torchrun --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --nproc_per_node=8 --master_port=62621 infer_mpdocvqa_dynamic_unsupervised_cls3.py --checkpoint /gruntdata/heyuan67/zqp/qp_lm_models/mpdocvqa_dude_newsvqa_slidevqa/1e-6/qwen2-vl-7b-instruct/v11-20241212-001249/checkpoint-18868/ --datasets dude_mpdocvqa_val --max-pixels-low 200704 --max-pixels-high 802816 --out-dir ./res/dude-mpdoc-vqa --dynamic --select-mode topk --select-image-num 5
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets:datasets:  ['dude_mpdocvqa_val']['dude_mpdocvqa_val']

datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
datasets: ['dude_mpdocvqa_val']
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.35it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.37it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.83it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.39it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.10it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.14it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.16it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.15it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.18it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.17it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.97it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.92it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.63it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.68it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.66it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.64it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.66it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  4.70it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.17it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.02it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  5.15it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.87it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.87it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.84it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.82it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.90it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.60it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  4.87it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.19it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.88it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.88it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.85it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.97it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.83it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.87it/s]
0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]0it [00:00, ?it/s]1it [00:01,  1.26s/it]2it [00:02,  1.24s/it]1it [00:01,  1.64s/it]1it [00:02,  2.19s/it]2it [00:02,  1.09s/it]1it [00:02,  2.44s/it]2it [00:02,  1.10s/it]3it [00:02,  1.16it/s]1it [00:03,  3.21s/it]4it [00:03,  1.45it/s]5it [00:03,  1.67it/s]2it [00:03,  1.74s/it]3it [00:05,  1.87s/it]1it [00:04,  4.57s/it]1it [00:04,  4.44s/it]3it [00:04,  1.59s/it]2it [00:04,  2.12s/it]1it [00:04,  4.89s/it]2it [00:05,  2.54s/it]2it [00:05,  2.19s/it]3it [00:05,  1.48s/it]4it [00:05,  1.12s/it]3it [00:05,  1.41s/it]3it [00:05,  1.39s/it]2it [00:05,  2.57s/it]4it [00:06,  1.07s/it]5it [00:05,  1.01s/it]3it [00:05,  1.79s/it]4it [00:06,  1.27s/it]4it [00:07,  1.95s/it]4it [00:06,  1.13s/it]6it [00:06,  1.20it/s]4it [00:06,  1.34s/it]3it [00:06,  1.77s/it]6it [00:06,  1.44s/it]5it [00:07,  1.01s/it]4it [00:07,  1.25s/it]7it [00:07,  1.08s/it]5it [00:07,  1.27s/it]5it [00:07,  1.15s/it]6it [00:07,  1.00s/it]5it [00:08,  1.41s/it]6it [00:08,  1.11s/it]7it [00:08,  1.33s/it]5it [00:09,  2.25s/it]7it [00:08,  1.05s/it]6it [00:09,  1.35s/it]6it [00:09,  1.38s/it]5it [00:09,  1.60s/it]8it [00:09,  1.08s/it]8it [00:09,  1.11it/s]7it [00:09,  1.10s/it]9it [00:09,  1.31it/s]8it [00:10,  1.65s/it]8it [00:10,  1.01it/s]9it [00:10,  1.10s/it]6it [00:11,  2.07s/it]9it [00:10,  1.29it/s]7it [00:10,  1.45s/it]6it [00:10,  1.60s/it]7it [00:11,  1.63s/it]10it [00:11,  1.02s/it]8it [00:11,  1.21s/it]7it [00:11,  1.29s/it]9it [00:11,  1.66s/it]9it [00:12,  1.06it/s]8it [00:11,  1.02it/s]7it [00:13,  1.85s/it]8it [00:12,  1.44s/it]9it [00:12,  1.18it/s]9it [00:12,  1.11s/it]8it [00:13,  1.44s/it]10it [00:12,  1.46it/s]10it [00:12,  1.18s/it]10it [00:13,  1.01it/s]11it [00:13,  1.67it/s]10it [00:13,  1.61s/it]11it [00:13,  1.01it/s]10it [00:13,  1.57s/it]11it [00:13,  1.41s/it]11it [00:14,  1.40s/it]12it [00:14,  1.17s/it]12it [00:14,  1.01s/it]11it [00:14,  1.18s/it]12it [00:14,  1.19it/s]13it [00:14,  1.02it/s]11it [00:14,  1.52s/it]10it [00:14,  1.48s/it]13it [00:14,  1.41it/s]9it [00:15,  1.72s/it]13it [00:15,  1.12it/s]14it [00:15,  1.29it/s]14it [00:15,  1.39it/s]15it [00:15,  1.53it/s]12it [00:15,  1.12s/it]12it [00:15,  1.40s/it]12it [00:15,  1.32s/it]15it [00:15,  1.60it/s]13it [00:16,  1.14it/s]16it [00:16,  1.45it/s]13it [00:16,  1.24s/it]10it [00:17,  1.65s/it]17it [00:16,  1.75it/s]11it [00:16,  1.59s/it]14it [00:17,  1.09s/it]18it [00:17,  1.62it/s]13it [00:17,  1.41s/it]11it [00:18,  1.41s/it]14it [00:17,  1.08s/it]19it [00:17,  1.75it/s]14it [00:18,  1.22s/it]14it [00:17,  1.37s/it]15it [00:17,  1.04it/s]12it [00:18,  1.17s/it]15it [00:18,  1.09s/it]16it [00:18,  1.20it/s]13it [00:19,  1.06it/s]12it [00:18,  1.64s/it]15it [00:18,  1.06s/it]16it [00:19,  1.42s/it]15it [00:19,  1.21s/it]16it [00:19,  1.07it/s]14it [00:20,  1.01it/s]20it [00:19,  1.06it/s]17it [00:20,  1.18it/s]21it [00:19,  1.31it/s]13it [00:19,  1.57s/it]18it [00:20,  1.36it/s]16it [00:20,  1.22s/it]19it [00:20,  1.64it/s]16it [00:20,  1.54s/it]17it [00:20,  1.34s/it]22it [00:20,  1.19it/s]17it [00:21,  1.62s/it]17it [00:21,  1.18s/it]15it [00:22,  1.27s/it]18it [00:21,  1.25s/it]20it [00:21,  1.43it/s]14it [00:21,  1.61s/it]15it [00:22,  1.26s/it]16it [00:23,  1.12s/it]23it [00:22,  1.03it/s]16it [00:22,  1.02it/s]24it [00:22,  1.18it/s]21it [00:23,  1.08it/s]18it [00:23,  1.59s/it]25it [00:23,  1.35it/s]17it [00:23,  1.75s/it]17it [00:24,  1.14s/it]17it [00:23,  1.01it/s]18it [00:23,  1.26it/s]18it [00:23,  1.65s/it]18it [00:24,  1.49s/it]19it [00:24,  1.50s/it]19it [00:24,  1.73s/it]22it [00:24,  1.09s/it]18it [00:25,  1.22s/it]19it [00:24,  1.22s/it]20it [00:24,  1.21s/it]26it [00:24,  1.04s/it]19it [00:25,  1.01s/it]21it [00:25,  1.11s/it]20it [00:25,  1.14it/s]23it [00:26,  1.21s/it]22it [00:26,  1.16it/s]19it [00:26,  1.85s/it]20it [00:26,  1.31s/it]24it [00:26,  1.06s/it]27it [00:26,  1.24s/it]19it [00:27,  1.44s/it]20it [00:26,  1.46s/it]23it [00:26,  1.19it/s]20it [00:26,  1.99s/it]21it [00:26,  1.12s/it]25it [00:27,  1.17it/s]24it [00:27,  1.29it/s]21it [00:27,  1.18s/it]25it [00:27,  1.51it/s]28it [00:28,  1.29s/it]21it [00:28,  1.45s/it]22it [00:28,  1.20s/it]22it [00:28,  1.12s/it]21it [00:28,  1.90s/it]20it [00:29,  1.60s/it]23it [00:28,  1.06it/s]22it [00:29,  1.45s/it]21it [00:30,  1.26s/it]24it [00:29,  1.22it/s]29it [00:29,  1.33s/it]25it [00:29,  1.39it/s]30it [00:29,  1.02s/it]22it [00:29,  1.46s/it]23it [00:29,  1.17s/it]22it [00:30,  1.13s/it]23it [00:31,  1.10it/s]26it [00:30,  1.62s/it]26it [00:30,  1.26s/it]24it [00:30,  1.03s/it]25it [00:30,  1.22it/s]24it [00:32,  1.06it/s]23it [00:31,  1.74s/it]26it [00:31,  1.34it/s]27it [00:31,  1.18s/it]25it [00:32,  1.26it/s]31it [00:31,  1.33s/it]28it [00:31,  1.07it/s]27it [00:32,  1.57s/it]28it [00:32,  1.22s/it]23it [00:32,  1.87s/it]26it [00:32,  1.41s/it]27it [00:33,  1.09s/it]24it [00:33,  1.78s/it]29it [00:33,  1.24s/it]27it [00:33,  1.18s/it]25it [00:33,  1.35s/it]32it [00:33,  1.55s/it]29it [00:33,  1.28s/it]28it [00:34,  1.03it/s]30it [00:34,  1.10s/it]33it [00:34,  1.26s/it]24it [00:34,  1.87s/it]29it [00:34,  1.26it/s]26it [00:35,  1.38s/it]25it [00:35,  1.47s/it]34it [00:35,  1.11s/it]28it [00:35,  1.49s/it]30it [00:35,  1.15it/s]35it [00:35,  1.12it/s]30it [00:35,  1.42s/it]26it [00:35,  1.60s/it]31it [00:36,  1.26it/s]31it [00:36,  1.15s/it]26it [00:36,  1.39s/it]36it [00:36,  1.23it/s]29it [00:36,  1.28s/it]31it [00:36,  1.39s/it]27it [00:37,  1.51s/it]27it [00:36,  1.39s/it]30it [00:36,  1.08s/it]32it [00:37,  1.27it/s]32it [00:37,  1.05s/it]32it [00:37,  1.22s/it]31it [00:37,  1.18it/s]28it [00:37,  1.13s/it]33it [00:37,  1.54it/s]27it [00:37,  1.32s/it]32it [00:37,  1.45it/s]33it [00:37,  1.03it/s]33it [00:37,  1.08it/s]34it [00:37,  1.60it/s]34it [00:38,  1.29it/s]34it [00:38,  1.18it/s]28it [00:38,  1.15s/it]37it [00:38,  1.16s/it]29it [00:38,  1.11it/s]33it [00:38,  1.29it/s]35it [00:38,  1.34it/s]38it [00:38,  1.02s/it]29it [00:38,  1.29s/it]28it [00:40,  1.90s/it]35it [00:39,  1.09it/s]35it [00:39,  1.12it/s]39it [00:39,  1.06it/s]30it [00:39,  1.04s/it]36it [00:40,  1.18it/s]34it [00:39,  1.02it/s]31it [00:40,  1.22it/s]30it [00:40,  1.28s/it]35it [00:40,  1.27it/s]40it [00:40,  1.09it/s]31it [00:40,  1.07s/it]41it [00:40,  1.34it/s]36it [00:41,  1.28it/s]37it [00:41,  1.01s/it]36it [00:41,  1.23s/it]32it [00:41,  1.04it/s]29it [00:42,  2.03s/it]37it [00:41,  1.06it/s]36it [00:41,  1.41s/it]38it [00:42,  1.05it/s]30it [00:43,  1.66s/it]32it [00:42,  1.26s/it]42it [00:42,  1.04s/it]37it [00:42,  1.04s/it]43it [00:42,  1.22it/s]38it [00:43,  1.17it/s]39it [00:43,  1.02it/s]38it [00:43,  1.16s/it]33it [00:43,  1.23s/it]44it [00:43,  1.36it/s]39it [00:43,  1.11it/s]39it [00:43,  1.30it/s]31it [00:44,  1.59s/it]34it [00:43,  1.01it/s]40it [00:44,  1.11it/s]40it [00:44,  1.29it/s]45it [00:44,  1.37it/s]35it [00:44,  1.16it/s]41it [00:44,  1.30it/s]37it [00:44,  1.81s/it]33it [00:44,  1.54s/it]40it [00:44,  1.13it/s]41it [00:44,  1.29it/s]32it [00:46,  1.52s/it]34it [00:45,  1.23s/it]41it [00:45,  1.39it/s]42it [00:45,  1.54it/s]46it [00:45,  1.11it/s]42it [00:45,  1.11it/s]47it [00:45,  1.38it/s]38it [00:45,  1.70s/it]42it [00:46,  1.33it/s]36it [00:46,  1.16s/it]48it [00:46,  1.50it/s]39it [00:46,  1.34s/it]43it [00:46,  1.07it/s]37it [00:46,  1.07it/s]33it [00:47,  1.55s/it]40it [00:46,  1.04s/it]43it [00:46,  1.10it/s]35it [00:46,  1.39s/it]44it [00:47,  1.31it/s]38it [00:47,  1.24it/s]49it [00:47,  1.39it/s]45it [00:47,  1.54it/s]39it [00:47,  1.40it/s]50it [00:47,  1.43it/s]44it [00:47,  1.05it/s]40it [00:48,  1.52it/s]36it [00:48,  1.36s/it]45it [00:48,  1.18it/s]41it [00:48,  1.29s/it]41it [00:48,  1.60it/s]34it [00:49,  1.68s/it]46it [00:48,  1.46it/s]46it [00:48,  1.12it/s]43it [00:48,  1.40s/it]35it [00:50,  1.36s/it]42it [00:49,  1.45it/s]37it [00:49,  1.36s/it]51it [00:49,  1.03s/it]44it [00:49,  1.27s/it]47it [00:50,  1.00s/it]36it [00:51,  1.18s/it]52it [00:50,  1.15it/s]47it [00:50,  1.11it/s]42it [00:50,  1.49s/it]48it [00:51,  1.03it/s]43it [00:51,  1.25s/it]43it [00:51,  1.01s/it]37it [00:52,  1.24s/it]45it [00:51,  1.38s/it]48it [00:51,  1.09s/it]44it [00:51,  1.06s/it]38it [00:51,  1.67s/it]53it [00:52,  1.28s/it]49it [00:52,  1.11s/it]45it [00:52,  1.00it/s]46it [00:52,  1.34s/it]46it [00:53,  1.26it/s]54it [00:53,  1.15s/it]47it [00:53,  1.12s/it]47it [00:53,  1.49it/s]39it [00:53,  1.71s/it]49it [00:53,  1.44s/it]38it [00:55,  1.63s/it]50it [00:54,  1.28s/it]44it [00:54,  1.54s/it]50it [00:54,  1.13s/it]39it [00:55,  1.26s/it]40it [00:55,  1.04s/it]51it [00:54,  1.03it/s]55it [00:55,  1.37s/it]51it [00:55,  1.21s/it]48it [00:55,  1.33s/it]52it [00:55,  1.29it/s]40it [00:55,  1.73s/it]48it [00:55,  1.10s/it]52it [00:55,  1.01it/s]45it [00:55,  1.65s/it]53it [00:56,  1.22it/s]49it [00:56,  1.36s/it]54it [00:56,  1.24it/s]53it [00:57,  1.06s/it]56it [00:56,  1.53s/it]41it [00:58,  1.44s/it]49it [00:57,  1.37s/it]57it [00:57,  1.24s/it]41it [00:57,  1.83s/it]46it [00:58,  1.77s/it]54it [00:58,  1.20s/it]50it [00:58,  1.30s/it]55it [00:58,  1.05it/s]58it [00:59,  1.32s/it]51it [00:59,  1.03s/it]42it [00:59,  1.73s/it]50it [00:59,  1.74s/it]56it [00:59,  1.28it/s]42it [01:00,  1.63s/it]52it [00:59,  1.23it/s]51it [00:59,  1.34s/it]55it [00:59,  1.44s/it]47it [00:59,  1.74s/it]59it [00:59,  1.13s/it]43it [00:59,  1.41s/it]43it [01:00,  1.26s/it]57it [00:59,  1.40it/s]60it [01:00,  1.13it/s]44it [01:01,  1.03it/s]48it [01:00,  1.38s/it]61it [01:00,  1.38it/s]45it [01:01,  1.19it/s]58it [01:00,  1.35it/s]52it [01:01,  1.39s/it]46it [01:02,  1.30it/s]53it [01:01,  1.15s/it]62it [01:01,  1.13it/s]49it [01:01,  1.40s/it]53it [01:01,  1.15s/it]47it [01:03,  1.29it/s]59it [01:02,  1.07it/s]54it [01:02,  1.11it/s]50it [01:02,  1.15s/it]44it [01:02,  1.82s/it]55it [01:02,  1.26it/s]48it [01:03,  1.27it/s]56it [01:03,  1.97s/it]56it [01:02,  1.48it/s]49it [01:04,  1.41it/s]45it [01:03,  1.56s/it]57it [01:03,  1.53it/s]51it [01:03,  1.27s/it]46it [01:03,  1.19s/it]50it [01:05,  1.45it/s]54it [01:04,  1.62s/it]58it [01:04,  1.68it/s]57it [01:04,  1.77s/it]63it [01:04,  1.39s/it]47it [01:04,  1.02s/it]52it [01:04,  1.08s/it]59it [01:04,  1.81it/s]64it [01:04,  1.06s/it]60it [01:04,  1.43s/it]51it [01:05,  1.41it/s]65it [01:04,  1.19it/s]60it [01:05,  1.61it/s]53it [01:05,  1.07s/it]61it [01:05,  1.27s/it]55it [01:05,  1.66s/it]58it [01:06,  1.73s/it]66it [01:05,  1.14it/s]62it [01:05,  1.00s/it]54it [01:05,  1.15it/s]61it [01:05,  1.62it/s]48it [01:06,  1.25s/it]49it [01:06,  1.03it/s]67it [01:06,  1.20it/s]52it [01:07,  1.12s/it]63it [01:06,  1.02s/it]56it [01:07,  1.53s/it]68it [01:07,  1.32it/s]53it [01:08,  1.05it/s]57it [01:07,  1.22s/it]69it [01:07,  1.43it/s]58it [01:07,  1.05it/s]50it [01:08,  1.14s/it]59it [01:08,  1.19it/s]55it [01:08,  1.37s/it]70it [01:08,  1.37it/s]59it [01:08,  2.04s/it]51it [01:08,  1.02s/it]62it [01:08,  1.36s/it]60it [01:09,  1.55s/it]60it [01:09,  1.28it/s]71it [01:09,  1.41it/s]61it [01:09,  1.25s/it]72it [01:09,  1.42it/s]63it [01:09,  1.22s/it]54it [01:11,  1.47s/it]64it [01:10,  1.01it/s]64it [01:10,  1.74s/it]52it [01:10,  1.18s/it]73it [01:10,  1.52it/s]62it [01:10,  1.18s/it]61it [01:10,  1.09s/it]56it [01:11,  1.80s/it]62it [01:11,  1.09it/s]55it [01:12,  1.44s/it]65it [01:11,  1.07s/it]53it [01:11,  1.24s/it]63it [01:11,  1.32it/s]64it [01:12,  1.61it/s]66it [01:12,  1.10it/s]56it [01:13,  1.32s/it]54it [01:12,  1.11s/it]67it [01:12,  1.26it/s]57it [01:12,  1.72s/it]65it [01:12,  1.49it/s]63it [01:13,  1.55s/it]74it [01:13,  1.29s/it]65it [01:13,  2.09s/it]57it [01:14,  1.24s/it]55it [01:13,  1.07s/it]64it [01:13,  1.26s/it]75it [01:13,  1.02s/it]66it [01:13,  1.41it/s]76it [01:14,  1.15it/s]58it [01:14,  1.76s/it]68it [01:14,  1.18s/it]65it [01:15,  1.35s/it]59it [01:15,  1.38s/it]69it [01:15,  1.01it/s]66it [01:15,  2.12s/it]67it [01:15,  1.05s/it]66it [01:15,  1.12s/it]60it [01:15,  1.21s/it]67it [01:16,  1.13it/s]70it [01:16,  1.06it/s]56it [01:16,  1.58s/it]61it [01:16,  1.03it/s]58it [01:17,  1.75s/it]68it [01:16,  1.24it/s]77it [01:16,  1.40s/it]62it [01:16,  1.17it/s]67it [01:17,  1.94s/it]68it [01:17,  1.17s/it]78it [01:16,  1.08s/it]59it [01:18,  1.53s/it]68it [01:17,  1.52s/it]69it [01:17,  1.01s/it]69it [01:18,  1.01s/it]71it [01:18,  1.27s/it]60it [01:19,  1.36s/it]57it [01:18,  1.79s/it]70it [01:18,  1.18it/s]70it [01:18,  1.02s/it]71it [01:19,  1.09it/s]63it [01:19,  1.37s/it]72it [01:19,  1.32s/it]73it [01:19,  1.04s/it]79it [01:19,  1.66s/it]72it [01:20,  1.07it/s]64it [01:20,  1.20s/it]58it [01:20,  1.78s/it]61it [01:21,  1.55s/it]71it [01:20,  1.21s/it]80it [01:20,  1.36s/it]74it [01:20,  1.02it/s]59it [01:21,  1.57s/it]62it [01:22,  1.44s/it]73it [01:21,  1.07s/it]72it [01:21,  1.16s/it]81it [01:21,  1.33s/it]63it [01:23,  1.19s/it]69it [01:22,  2.49s/it]65it [01:22,  1.57s/it]70it [01:22,  1.93s/it]75it [01:22,  1.31s/it]64it [01:23,  1.04s/it]73it [01:23,  1.18s/it]66it [01:23,  1.22s/it]74it [01:23,  1.21s/it]76it [01:23,  1.02s/it]71it [01:23,  1.52s/it]82it [01:23,  1.43s/it]77it [01:23,  1.18it/s]78it [01:24,  1.31it/s]72it [01:24,  1.31s/it]74it [01:24,  1.32s/it]65it [01:25,  1.28s/it]60it [01:24,  2.10s/it]83it [01:24,  1.34s/it]73it [01:25,  1.20s/it]67it [01:25,  1.53s/it]60it [01:25,  1.43s/it]
84it [01:25,  1.18s/it][rank7]: Traceback (most recent call last):
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls3.py", line 550, in <module>
[rank7]:     evaluate_chat_model(model, processor, image_process_args, args.select_mode, args.select_image_num)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls3.py", line 367, in evaluate_chat_model
[rank7]:     page_pred = predict_page(inputs[0].to(model.device),
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/infer_mpdocvqa_dynamic_unsupervised_cls3.py", line 220, in predict_page
[rank7]:     image_embeds = qwen2vl_generation_model.visual(pixel_values, grid_thw=image_grid_thw).to(inputs_embeds.device)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 1197, in forward
[rank7]:     hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 435, in forward
[rank7]:     hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mntnlp/zqp/physical_report/Qwen2-VL/model/modeling_qwen2_vl_unsupervised_cls2.py", line 323, in forward
[rank7]:     return self.fc2(self.act(self.fc1(x)))
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/root/miniconda3/lib/python3.10/site-packages/transformers/activations.py", line 96, in forward
[rank7]:     return input * torch.sigmoid(1.702 * input)
[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 7 has a total capacity of 79.35 GiB of which 162.19 MiB is free. Process 507445 has 79.18 GiB memory in use. Of the allocated memory 75.35 GiB is allocated by PyTorch, and 3.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
74it [01:25,  1.01s/it]75it [01:26,  1.31s/it]68it [01:26,  1.28s/it]69it [01:26,  1.02s/it]79it [01:26,  1.23s/it]66it [01:27,  1.48s/it]75it [01:26,  1.95s/it]80it [01:26,  1.04it/s]70it [01:27,  1.12it/s]W1216 17:54:30.980000 140123587500928 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 174933 closing signal SIGTERM
W1216 17:54:30.980000 140123587500928 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 174934 closing signal SIGTERM
W1216 17:54:30.981000 140123587500928 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 174935 closing signal SIGTERM
W1216 17:54:30.983000 140123587500928 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 174936 closing signal SIGTERM
W1216 17:54:30.984000 140123587500928 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 174937 closing signal SIGTERM
W1216 17:54:30.985000 140123587500928 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 174938 closing signal SIGTERM
W1216 17:54:30.985000 140123587500928 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 174939 closing signal SIGTERM
E1216 17:54:33.193000 140123587500928 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 7 (pid: 174940) of binary: /root/miniconda3/bin/python
Traceback (most recent call last):
  File "/root/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/root/miniconda3/lib/python3.10/site-packages/zhenjin_utils/import_hooks/torchrun_args_hook.py", line 28, in new_run
    return module.old_run(*func_args, **func_kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
infer_mpdocvqa_dynamic_unsupervised_cls3.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-16_17:54:30
  host      : kmaker-kmaker-033145099100
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 174940)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-dude-mpdocvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-newsvqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-newsvqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-newsvqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-newsvqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-newsvqa-val-unsupervised-cls-v4-top5_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-slidevqa-val ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-slidevqa-val-unsupervised-cls-v3-top3 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-slidevqa-val-unsupervised-cls-v3-top5 ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-slidevqa-val-unsupervised-cls-v4-top3_mix ']'
+ '[' vqa-dude-mpdocvqa-val-unsupervised-cls-v3-top5 == vqa-slidevqa-val-unsupervised-cls-v4-top5_mix ']'
